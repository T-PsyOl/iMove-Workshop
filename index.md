## Workshop at iMove 2025 (LÃ¼beck)

This repository accompanies the hands-on session presented at  
**iMove 2025 â€“ International Conference on Movement and Computing**  
LÃ¼beck, Germany  
<https://imove2025.org/>

### **Session Title**
**The do's and don'ts of multimodal data acquisition in everyday life**

### **Presenters**
- **Martin Bleichner** (University of Oldenburg)  
- **Melanie Klapprott** (University of Oldenburg)

---

## Overview

In this hands-on tutorial, we explore practical strategies for collecting
multimodal physiological and behavioral data in everyday life environments.
We discuss pitfalls, technical challenges, and best practices when working
with mobile EEG, wearable sensors, and event-based data streams.

The tools in this repository (MATLAB scripts, examples, and documentation)
are designed to give participants a starting point for building their own
multimodal recording and analysis pipelines.

---


## Materials Included in This Repository

### **1. `keyboardEventScript.m`**
A minimal LSL event stream generator that sends keyboard press and release
events as markers.  
Useful for prototyping timing, testing experimental pipelines, and validating
LSL connections.  
Created by **Martin Bleichner (2025)**.

> *Note:* No guarantee of precise event timing â€” this tool is intended for
training and demonstration purposes.
> 
<a href="https://raw.githubusercontent.com/T-PsyOl/iMove-Workshop/main/keyboardEventScript.m"
   download="keyboardEventScript.m"
   style="display:inline-block;
          padding:10px 16px;
          background:#007acc;
          color:white;
          border-radius:6px;
          text-decoration:none;
          font-weight:600;">
  ğŸ“¥ Get keyboardEventScript.m
</a>


### **2. `iMoveScript.m`**
A script that:
- Loads XDF files  
- Identifies keyboard streams  
- Aligns multiple participants on a shared time axis  
- Visualizes pressâ€“release durations as horizontal bars  
- Generates audio sonifications of the performance  
- Computes synchrony and asynchrony between players  

Designed for illustrating timing, coordination, and event structure in
multimodal data.

<a href="https://raw.githubusercontent.com/T-PsyOl/iMove-Workshop/main/iMoveScript.m"
   download="iMoveScript.m"
   style="display:inline-block;
          padding:10px 16px;
          background:#007acc;
          color:white;
          border-radius:6px;
          text-decoration:none;
          font-weight:600;">
  ğŸ“¥ Get iMoveScript.m
</a>

## Prerequisites: MATLAB + EEGLAB + LSL Libraries
To ensure that the scripts in this repository run correctly, please complete the following setup steps.

1. Install MATLAB

2. Install EEGLAB
Download EEGLAB from the official website:
<https://sccn.ucsd.edu/eeglab/download.php>

3. Start EEGLAB in MATLAB

Launch MATLAB and run:

eeglab

4. Install the LSL MATLAB Viewer Plugin

Inside EEGLAB:

Go to File â†’ Manage EEGLAB Extensions

Search for: "LSL"

Install the xdfimport plugin
Install the lsl matlab viewer plugin

Restart EEGLAB if prompted

This plugin automatically installs the required LSL dependencies.

5. Initialize the LSL MATLAB Viewer Once

After installation, run the LSL Matlab Viewer:
Go to File â†’ LSL Matlab Viewer

or type in 
vis_stream

This ensures all dependencies are correctly added to MATLABâ€™s path.

### Optional: Install LabRecorder (for your own recordings)

If you want to record your own LSL data (e.g., keyboard events, EEG streams, sensors), get the  LabRecorder:

<https://github.com/labstreaminglayer/App-LabRecorder/releases>

Choose the appropriate version for your operating system, download it, and unzip it.
No installation is required â€” just run the LabRecorder executable.

## References

Kothe, Christian, Seyed Yahya Shirazi, Tristan Stenner, et al.
â€œThe Lab Streaming Layer for Synchronized Multimodal Recording.â€
Imaging Neuroscience, online ahead of print, 18 August 2025.
<https://doi.org/10.1162/IMAG.a.136>

Blum, Sarah, Daniel HÃ¶lle, Martin Georg Bleichner, and Stefan Debener.
â€œPocketable Labs for Everyone: Synchronized Multi-Sensor Data Streaming and Recording on Smartphones with the Lab Streaming Layer.â€
Sensors 21, no. 23 (2021): 23.
<https://doi.org/10.3390/s21238135>

Klapprott, Melanie, and Stefan Debener.
â€œMobile EEG for the Study of Cognitive-Motor Interference during Swimming?â€
Frontiers in Human Neuroscience 18 (2024): 1466853.
<https://doi.org/10.3389/fnhum.2024.1466853>

HÃ¶lle, Daniel, Sarah Blum, Sven Kissner, Stefan Debener, and Martin G. Bleichner.
â€œReal-Time Audio Processing of Real-Life Soundscapes for EEG Analysis: ERPs Based on Natural Sound Onsets.â€
Frontiers in Neuroergonomics 3 (2022).
<https://doi.org/10.3389/fnrgo.2022.793061>

HÃ¶lle, Daniel, Joost Meekes, and Martin G. Bleichner.
â€œMobile Ear-EEG to Study Auditory Attention in Everyday Life.â€
Behavior Research Methods 53, no. 5 (2021): 2025â€“2036.
<https://doi.org/10.3758/s13428-021-01538-0>

HÃ¶lle, Daniel, and Martin G. Bleichner.
â€œSmartphone-Based Ear-Electroencephalography to Study Sound Processing in Everyday Life.â€
European Journal of Neuroscience 58, no. 7 (2023): 3671â€“3685.
<https://doi.org/10.1111/ejn.16124>

Papin, Lara J., Manik Esche, Joanna E. M. Scanlon, Nadine S. J. Jacobsen, and Stefan Debener.
â€œInvestigating Cognitive-Motor Effects during Slacklining Using Mobile EEG.â€
Frontiers in Human Neuroscience 18 (May 2024).
<https://doi.org/10.3389/fnhum.2024.1382959>

Rosenkranz, Marc, Timur Cetin, Verena N. Uslar, and Martin G. Bleichner.
â€œInvestigating the Attentional Focus to Workplace-Related Soundscapes in a Complex Audio-Visual-Motor Task Using EEG.â€
Frontiers in Neuroergonomics 3 (2023).
<https://doi.org/10.3389/fnrgo.2022.1062227>

Rosenkranz, Marc, Thorge Haupt, Manuela Jaeger, Verena N. Uslar, and Martin G. Bleichner.
â€œUsing Mobile EEG to Study Auditory Work Strain during Simulated Surgical Procedures.â€
Scientific Reports 14, no. 1 (2024): 24026.
<https://doi.org/10.1038/s41598-024-74946-9>

Korte, Silvia, Manuela Jaeger, Marc Rosenkranz, and Martin G. Bleichner.
â€œFrom Beeps to Streets: Unveiling Sensory Input and Relevance across Auditory Contexts.â€
Frontiers in Neuroergonomics 6 (April 2025): 1571356.
<https://doi.org/10.3389/fnrgo.2025.1571356>
